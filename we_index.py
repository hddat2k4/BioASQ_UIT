import torch, os, json, nltk, uuid
from tqdm import tqdm
from nltk.tokenize import sent_tokenize
nltk.download('punkt')

# --- Import th∆∞ vi·ªán c·∫ßn thi·∫øt ---
import weaviate
from langchain_core.documents import Document
from langchain.embeddings.base import Embeddings
from langchain.vectorstores import Weaviate  # S·ª≠ d·ª•ng vector store c·ªßa Weaviate t·ª´ LangChain
from sentence_transformers import SentenceTransformer

# --- Kh·ªüi t·∫°o embedding model ---
class SentenceTransformerEmbeddings(Embeddings):
    def __init__(self, model):
        self.model = model

    def embed_documents(self, texts):
        return self.model.encode(texts, show_progress_bar=True, convert_to_numpy=True).tolist()

    def embed_query(self, text):
        return self.model.encode(text, show_progress_bar=True, convert_to_numpy=True).tolist()

model_name = "BAAI/bge-small-en-v1.5"
device = 'cuda' if torch.cuda.is_available() else 'cpu'
raw_model = SentenceTransformer(model_name, device=device)
embeddings = SentenceTransformerEmbeddings(raw_model)

# --- K·∫øt n·ªëi ƒë·∫øn Weaviate vector store ---
# Gi·∫£ s·ª≠ Weaviate ƒëang ch·∫°y t·∫°i http://localhost:8080 (ƒëi·ªÅu n√†y ph√π h·ª£p v·ªõi c·∫•u h√¨nh Docker c·ªßa b·∫°n)
client = weaviate.Client("http://localhost:8080")

# --- T√πy ch·ªçn: Ki·ªÉm tra v√† t·∫°o schema (class) n·∫øu ch∆∞a t·ªìn t·∫°i ---
# Ch√∫ng ta ƒë·ªãnh nghƒ©a m·ªôt schema t√™n "Document" v·ªõi c√°c tr∆∞·ªùng "page_content" v√† "metadata"
schema = client.schema.get()
if not any(cls["class"] == "Document" for cls in schema.get("classes", [])):
    document_schema = {
        "class": "Document",
        "properties": [
            {"name": "page_content", "dataType": ["text"]},
            # N·∫øu mu·ªën l∆∞u metadata d∆∞·ªõi d·∫°ng text (chu·ªói JSON) th√¨ c·∫ßn th√™m tr∆∞·ªùng n√†y,
            # ho·∫∑c b·∫°n c√≥ th·ªÉ l∆∞u c√°c thu·ªôc t√≠nh ri√™ng bi·ªát n·∫øu mu·ªën.
            {"name": "metadata", "dataType": ["text"]},
        ],
        # "vectorizer": "none" c√≥ nghƒ©a l√† b·∫°n s·∫Ω cung c·∫•p vector embeddings b√™n ngo√†i (kh√¥ng d√πng vectorizer c·ªßa Weaviate)
        "vectorizer": "none"
    }
    client.schema.create_class(document_schema)
    print("‚úÖ ƒê√£ t·∫°o schema 'Document' trong Weaviate.")

# --- Kh·ªüi t·∫°o vector store s·ª≠ d·ª•ng Weaviate ---
# index_name ·ªü ƒë√¢y ch√≠nh l√† t√™n c·ªßa class/schema ("Document")
# text_key l√† tr∆∞·ªùng ch·ª©a n·ªôi dung ch√≠nh c·ªßa t√†i li·ªáu ("page_content")
# embedding_function l√† h√†m chuy·ªÉn ƒë·ªïi text th√†nh vector (·ªü ƒë√¢y s·ª≠ d·ª•ng embeddings.embed_query)
vector_store = Weaviate(
    client,
    index_name="Document",
    text_key="page_content",
    embedding_function=embeddings.embed_query
)

# --- Ti·ªÅn x·ª≠ l√Ω v√† indexing ---
# Gi·∫£ s·ª≠ th∆∞ m·ª•c 'pubmed_json_2025' ch·ª©a c√°c file JSON c·∫ßn index
dir = 'pubmed_json_2025'

for i in tqdm(range(12, 14), desc="Indexing pubmed JSON files"):
    file = f"pubmed25n{i:04d}.json"
    path = os.path.join(dir, file)

    with open(path, 'r', encoding="utf-8") as f:
        data = json.load(f)

    documents = []
    ids = []

    for article in data:
        title = article.get("title", "").strip()
        abstract = article.get("abstract", "").strip()
        if not title and not abstract:
            continue

        full_text = f"{title}\n{abstract}".strip()
        chunks = sent_tokenize(full_text)

        for idx, chunk in enumerate(chunks):
            chunk_id = str(uuid.uuid4())
            doc = Document(
                page_content=chunk,
                metadata={
                    "title": title,
                    "pmid": article.get("pmid", ""),
                    "has_abstract": bool(abstract),
                    "chunk": idx
                }
            )
            documents.append(doc)
            ids.append(chunk_id)

    print(f"üì• S·ªë document t·ª´ {file}: {len(documents)}")
    vector_store.add_documents(documents=documents, ids=ids)

print("‚úÖ Ho√†n t·∫•t indexing c√°c snippet v√†o Weaviate.")
